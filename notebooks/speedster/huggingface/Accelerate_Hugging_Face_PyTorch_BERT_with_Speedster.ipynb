{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef331be9",
   "metadata": {
    "id": "ef331be9"
   },
   "source": [
    "![nebullvm nebuly AI accelerate inference optimize DeepLearning](https://user-images.githubusercontent.com/38586138/201391643-a80407e5-2c28-409c-90c9-327795cd27e8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260653a",
   "metadata": {
    "id": "f260653a"
   },
   "source": [
    "# Accelerate Hugging Face PyTorch BERT with Speedster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf3af5",
   "metadata": {
    "id": "8bdf3af5"
   },
   "source": [
    "Hi and welcome ğŸ‘‹\n",
    "\n",
    "In this notebook we will discover how in just a few steps you can speed up the response time of deep learning model inference using the Speedster app from the open-source library nebullvm.\n",
    "\n",
    "With Speedster's latest API, you can speed up models up to 10 times without any loss of accuracy (option A), or accelerate them up to 20-30 times by setting a self-defined amount of accuracy/precision that you are willing to trade off to get even lower response time (option B). To accelerate your model, Speedster takes advantage of various optimization techniques such as deep learning compilers (in both option A and option B), quantization, half accuracy, and so on (option B).\n",
    "\n",
    "Let's jump to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62c7cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d527d63b",
   "metadata": {
    "id": "d527d63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cXXh1ifQ13mH",
   "metadata": {
    "id": "cXXh1ifQ13mH"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aljCHu14-H",
   "metadata": {
    "id": "48aljCHu14-H"
   },
   "source": [
    "Install Speedster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Install deep learning compilers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d53ce534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/nebuly-ai/nebullvm.git#subdirectory=apps/accelerate/speedster\n",
      "  Cloning https://github.com/nebuly-ai/nebullvm.git to /tmp/pip-req-build-j5jcezwo\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nebuly-ai/nebullvm.git /tmp/pip-req-build-j5jcezwo\n",
      "  Resolved https://github.com/nebuly-ai/nebullvm.git to commit fe6716f956e281076c90593c65935379bee6c992\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nebullvm>=0.7.3 in /usr/local/lib/python3.8/dist-packages (from speedster==0.1.3) (0.7.3)\n",
      "Requirement already satisfied: tabulate>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from speedster==0.1.3) (0.9.0)\n",
      "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (6.0)\n",
      "Requirement already satisfied: py-cpuinfo>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (9.0.0)\n",
      "Requirement already satisfied: tqdm>=4.36.0 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (4.64.1)\n",
      "Requirement already satisfied: loguru>=0.5.3 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (0.6.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (2.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (5.9.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (23.0)\n",
      "Requirement already satisfied: numpy<=1.23.1,>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from nebullvm>=0.7.3->speedster==0.1.3) (1.21.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->nebullvm>=0.7.3->speedster==0.1.3) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->nebullvm>=0.7.3->speedster==0.1.3) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->nebullvm>=0.7.3->speedster==0.1.3) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->nebullvm>=0.7.3->speedster==0.1.3) (3.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/nebuly-ai/nebullvm.git#subdirectory=apps/accelerate/speedster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "QFQh3BVr1-GO",
   "metadata": {
    "id": "QFQh3BVr1-GO"
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/nebuly-ai/nebullvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f34e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install speedster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795d6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y speedster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffbfa32",
   "metadata": {
    "id": "cffbfa32"
   },
   "outputs": [],
   "source": [
    "# !python -m nebullvm.installers.auto_installer  --backends huggingface-full-torch --compilers all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73072506",
   "metadata": {
    "id": "73072506"
   },
   "source": [
    "## Model and Dataset setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d55115",
   "metadata": {
    "id": "e4d55115"
   },
   "source": [
    "We chose BERT as the pre-trained model that we want to optimize. Let's download both the pre-trained model and the tokenizer from the Hugging Face model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d633cf21",
   "metadata": {
    "id": "d633cf21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', torchscript=True)\n",
    "\n",
    "# Move the model to gpu if available and set eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa0739",
   "metadata": {
    "id": "11aa0739"
   },
   "source": [
    "Let's create an example dataset with some random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbfeeb2",
   "metadata": {
    "id": "cbbfeeb2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sentences = [\n",
    "    \"Mars is the fourth planet from the Sun.\",\n",
    "    \"has a crust primarily composed of elements\",\n",
    "    \"However, it is unknown\",\n",
    "    \"can be viewed from Earth\",\n",
    "    \"It was the Romans\",\n",
    "]\n",
    "\n",
    "len_dataset = 100\n",
    "\n",
    "texts = []\n",
    "for _ in range(len_dataset):\n",
    "    n_times = random.randint(1, 30)\n",
    "    texts.append(\" \".join(random.choice(sentences) for _ in range(n_times)))\n",
    "encoded_inputs = [tokenizer(text, return_tensors=\"pt\") for text in texts]\n",
    "len(encoded_inputs),encoded_inputs[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17040431",
   "metadata": {
    "id": "17040431"
   },
   "source": [
    "## Speed up inference with Speedster: no metric drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddc21d",
   "metadata": {
    "id": "44ddc21d"
   },
   "source": [
    "It's now time of improving a bit the performance in terms of speed. Let's use `Speedster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a0120a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ttj/github/nebullvm/apps/accelerate/speedster/speedster/__init__.py'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speedster\n",
    "from speedster import optimize_model\n",
    "speedster.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76248033",
   "metadata": {
    "id": "76248033"
   },
   "source": [
    "Using Speedster is very simple and straightforward! Just use the `optimize_model` function and provide as input the model, some input data as example and the optimization time mode. Optionally a dynamic_info dictionary can be also provided, in order to support inputs with dynamic shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "zPC_EDwEJIM0",
   "metadata": {
    "id": "zPC_EDwEJIM0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-01-26 16:58:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1minside type <class 'speedster.root_op.SpeedsterRootOp'>\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:03\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mMissing Frameworks: tensorflow.\n",
      " Please install them to include them in the optimization pipeline.\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.0043879556655883785 sec/iter\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mframework is DeepLearningFramework.PYTORCH, will convert to onnx\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mself.conversion_op: <nebullvm.operations.conversions.converters.PytorchConverter object at 0x7f28323cbd90>, converted model will be saved at /tmp/tmpnhj7ieli/fp32\u001b[0m\n",
      "self.conversion_op: <nebullvm.operations.conversions.converters.PytorchConverter object at 0x7f28323cbd90>, converted model will be saved at /tmp/tmpnhj7ieli/fp32\n",
      "\u001b[32m2023-01-26 16:58:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1moptimizing converted model: PyTorchTransformerWrapper(\n",
      "  (core_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1moptimization_op <nebullvm.operations.optimizations.optimizers.PytorchOptimizer object at 0x7f28323cbdf0>\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0033483505249023438 sec/iter\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:39\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:39\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding). If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1moptimizing converted model: /tmp/tmpnhj7ieli/fp32/temp.onnx\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1moptimization_op <nebullvm.operations.optimizations.optimizers.ONNXOptimizer object at 0x7f28323cbe80>\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0029931068420410156 sec/iter\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-01-26 16:58:44\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\n",
      "[Speedster results on NVIDIA GeForce RTX 3090]\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ Metric      â”ƒ Original Model   â”ƒ Optimized Model   â”ƒ Improvement   â”ƒ\n",
      "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«\n",
      "â”ƒ backend     â”ƒ PYTORCH          â”ƒ ONNXRuntime       â”ƒ               â”ƒ\n",
      "â”ƒ latency     â”ƒ 0.0044 sec/batch â”ƒ 0.0030 sec/batch  â”ƒ 1.47x         â”ƒ\n",
      "â”ƒ throughput  â”ƒ 227.90 data/sec  â”ƒ 334.10 data/sec   â”ƒ 1.47x         â”ƒ\n",
      "â”ƒ model size  â”ƒ 438.03 MB        â”ƒ 438.23 MB         â”ƒ 0%            â”ƒ\n",
      "â”ƒ metric drop â”ƒ                  â”ƒ 0.0033            â”ƒ               â”ƒ\n",
      "â”ƒ techniques  â”ƒ                  â”ƒ fp16              â”ƒ               â”ƒ\n",
      "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n",
      "\n",
      "Max speed-up with your input parameters is 1.47x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic_info = {\n",
    "    \"inputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "        {0: 'batch', 1: 'num_tokens'},\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        {0: 'batch', 1: 'num_tokens'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "optimized_model = optimize_model(\n",
    "    model=model,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "#    ignore_compilers=[\"tensor RT\"],  #Â TensorRT does not work for this model\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "\n",
    "    dynamic_info=dynamic_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c6ab09",
   "metadata": {
    "id": "98c6ab09"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Move inputs to gpu if available\n",
    "encoded_inputs = [tokenizer(text, return_tensors=\"pt\").to(device) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b3b21",
   "metadata": {
    "id": "6e5b3b21"
   },
   "source": [
    "Let's run the prediction 100 times to calculate the average response time of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9284d1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ba9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, model_desc='original BERT'):\n",
    "    times = []\n",
    "\n",
    "    # Warmup for 30 iterations\n",
    "    for encoded_input in encoded_inputs[:30]:\n",
    "        with torch.no_grad():\n",
    "            final_out = model(**encoded_input)\n",
    "\n",
    "    # Benchmark\n",
    "    for encoded_input in encoded_inputs:\n",
    "        st = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            final_out = model(**encoded_input)\n",
    "        times.append(time.perf_counter()-st)\n",
    "    original_model_time = sum(times)/len(times)*1000\n",
    "    print(f\"Average response time for {model_desc}: {original_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3bc5c98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3bc5c98",
    "outputId": "e0596cf2-fa96-4c50-c012-f5cdab82e681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original BERT: 4.486726749601075 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, 'original BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2df98",
   "metadata": {
    "id": "12c2df98"
   },
   "source": [
    "Let's see the output of the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4892a905",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4892a905",
    "outputId": "68d9b65f-e2cc-4998-8047-c9091f977698"
   },
   "outputs": [],
   "source": [
    "# model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0a7a1",
   "metadata": {
    "id": "3db0a7a1"
   },
   "source": [
    "Let's run the prediction 100 times to calculate the average response time of the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e83997",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3e83997",
    "outputId": "7a416b14-f170-4df9-d416-026f06a7d980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized BERT (no metric drop): 2.9902880199369974 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, 'optimized BERT (no metric drop)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "743fb413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nebullvm.operations.inference_learners.huggingface.HuggingFaceInferenceLearner,\n",
       " HuggingFaceInferenceLearner(network_parameters=ModelParams(batch_size=1, input_infos=[<nebullvm.tools.base.InputInfo object at 0x7f9e96bbb880>, <nebullvm.tools.base.InputInfo object at 0x7f9e96bbb8e0>, <nebullvm.tools.base.InputInfo object at 0x7f9e96bbb940>], output_sizes=[(32, 768), (768,)], dynamic_info=DynamicAxisInfo(inputs=[{0: 'batch', 1: 'num_tokens'}, {0: 'batch', 1: 'num_tokens'}], outputs=[{0: 'batch', 1: 'num_tokens'}])), input_tfms=None, device=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimized_model), optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d884d61",
   "metadata": {
    "id": "0d884d61"
   },
   "source": [
    "Let's see the output of the optimized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75611b2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75611b2e",
    "outputId": "035d5c6d-fd7a-4506-af09-befcf9dd3b2d"
   },
   "outputs": [],
   "source": [
    "# optimized_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7f6fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'optimized_model'\n",
    "optimized_model.save(save_path)\n",
    "\n",
    "from nebullvm.operations.inference_learners.base import LearnerMetadata\n",
    "\n",
    "optimized_model_reload = LearnerMetadata.read(save_path).load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c371885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for reloaded optimized BERT (no metric drop): 35.16878379959962 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_reload, 'reloaded optimized BERT (no metric drop)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c477942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rich\n",
    "# rich.inspect(optimized_model, methods=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60e55ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rich\n",
    "# rich.inspect(optimized_model_reload, methods=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb60d8c",
   "metadata": {
    "id": "ceb60d8c"
   },
   "source": [
    "## Speed up inference with Speedster: metric drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1950d5",
   "metadata": {
    "id": "7b1950d5"
   },
   "source": [
    "This time we will use the `metric_drop_ths` argument to accept a little drop in terms of precision, in order to enable quantization and obtain an higher speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de5721d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de5721d8",
    "outputId": "c9efff21-f963-47ff-e83d-a44615f90a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-01-21 10:22:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU\u001b[0m\n",
      "\u001b[32m2023-01-21 10:22:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-01-21 10:22:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.004553823471069336 sec/iter\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0028252601623535156 sec/iter\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:03\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:03\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding). If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.002164602279663086 sec/iter\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.001886606216430664 sec/iter\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.11/attention/self/MatMul_1]\n",
      "\u001b[32m2023-01-21 10:23:22\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-01-21 10:23:44\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\n",
      "[Speedster results on NVIDIA GeForce RTX 3090]\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ Metric      â”ƒ Original Model   â”ƒ Optimized Model   â”ƒ Improvement   â”ƒ\n",
      "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«\n",
      "â”ƒ backend     â”ƒ PYTORCH          â”ƒ ONNXRuntime       â”ƒ               â”ƒ\n",
      "â”ƒ latency     â”ƒ 0.0046 sec/batch â”ƒ 0.0019 sec/batch  â”ƒ 2.41x         â”ƒ\n",
      "â”ƒ throughput  â”ƒ 219.60 data/sec  â”ƒ 530.05 data/sec   â”ƒ 2.41x         â”ƒ\n",
      "â”ƒ model size  â”ƒ 438.03 MB        â”ƒ 219.36 MB         â”ƒ -49%          â”ƒ\n",
      "â”ƒ metric drop â”ƒ                  â”ƒ 0.0081            â”ƒ               â”ƒ\n",
      "â”ƒ techniques  â”ƒ                  â”ƒ fp16              â”ƒ               â”ƒ\n",
      "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model=model,\n",
    "    input_data=encoded_inputs,\n",
    "    optimization_time=\"constrained\",\n",
    "    ignore_compilers=[\"tensor_rt\", \"tvm\"],\n",
    "    dynamic_info=dynamic_info,\n",
    "    metric_drop_ths=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fbfe6fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fbfe6fa",
    "outputId": "ada293f5-9b54-4186-8e48-74b994d4b797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original BERT: 4.694141010113526 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        final_out = model(**encoded_input)\n",
    "    times.append(time.perf_counter()-st)\n",
    "original_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for original BERT: {original_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bae1cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model):\n",
    "    times = []\n",
    "\n",
    "    # Warmup for 30 iterations\n",
    "    for encoded_input in encoded_inputs[:30]:\n",
    "        with torch.no_grad():\n",
    "            final_out = model(**encoded_input)\n",
    "\n",
    "    # Benchmark\n",
    "    for encoded_input in encoded_inputs:\n",
    "        st = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            final_out = model(**encoded_input)\n",
    "        times.append(time.perf_counter()-st)\n",
    "    original_model_time = sum(times)/len(times)*1000\n",
    "    print(f\"Average response time for original BERT: {original_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f89b7e6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f89b7e6d",
    "outputId": "51e497e1-a533-432d-d68e-b373f0ef69cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3959, -0.2622,  0.2219,  ..., -0.8757,  1.0242,  0.1929],\n",
       "          [-0.1232,  1.0914,  0.7587,  ..., -0.7951,  1.3011,  0.3600],\n",
       "          [-1.6476, -0.3754,  0.5519,  ...,  0.0806,  0.6507,  0.6131],\n",
       "          ...,\n",
       "          [-0.6952, -0.5882, -0.1518,  ..., -0.2717,  0.7255, -0.5284],\n",
       "          [-1.0378, -1.0159, -0.3399,  ...,  0.5535,  0.9189, -0.3754],\n",
       "          [ 0.1252,  0.1695, -0.2149,  ...,  0.4608, -0.2041, -0.1065]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-0.4473, -0.7220, -0.9987,  0.8181,  0.9683, -0.6342, -0.2493,  0.6438,\n",
       "          -0.9947, -0.9992, -0.9269,  0.9948,  0.6546,  0.9041, -0.1536, -0.7647,\n",
       "          -0.7596, -0.6340,  0.3766,  0.9080,  0.4105,  1.0000, -0.8801,  0.6671,\n",
       "           0.5953,  0.9994, -0.8529,  0.4076,  0.6064,  0.4705, -0.2271,  0.5080,\n",
       "          -0.9505, -0.3206, -0.9993, -0.8728,  0.7812,  0.2545, -0.1004, -0.2399,\n",
       "          -0.0988,  0.6550,  1.0000,  0.0055,  0.8167, -0.0669, -1.0000,  0.5785,\n",
       "          -0.1695,  0.9978,  0.9921,  0.9986,  0.5614,  0.5627,  0.7525, -0.9059,\n",
       "           0.1232,  0.3975, -0.5901, -0.5176, -0.5332,  0.5376, -0.9897,  0.0602,\n",
       "           0.9989,  0.9930, -0.6485, -0.5344, -0.2740, -0.0871, -0.1199,  0.3923,\n",
       "          -0.5982, -0.4238,  0.9845,  0.5905, -0.6776,  1.0000, -0.7444, -0.8252,\n",
       "           0.9967,  0.9880,  0.7953, -0.9481,  0.9652, -1.0000,  0.7931, -0.4254,\n",
       "          -0.7941,  0.5117,  0.6883, -0.4835,  0.9794,  0.7789, -0.6443, -0.8615,\n",
       "          -0.5161, -0.9981, -0.5555, -0.7454,  0.4873, -0.5542, -0.7418, -0.3545,\n",
       "           0.5978, -0.7145,  0.1292,  0.8312,  0.5765,  0.3555,  0.6973, -0.5938,\n",
       "           0.7049, -0.5051,  0.6046, -0.4614, -0.8867, -0.7160, -0.8241,  0.1554,\n",
       "          -0.7812, -0.5042, -0.1074, -0.9259,  0.7657, -0.5789, -0.9989, -1.0000,\n",
       "          -0.8741, -0.8764, -0.6625, -0.5700, -0.8099, -0.8158,  0.6154,  0.4726,\n",
       "           0.5245,  0.9998, -0.6360,  0.6328, -0.8354, -0.9587,  0.9788, -0.5679,\n",
       "           0.9629, -0.1218,  0.0244,  0.2344, -0.9143,  0.7670, -0.9580, -0.4881,\n",
       "          -0.9868, -0.1250, -0.2956,  0.5394, -0.9664, -0.9989, -0.8854, -0.2883,\n",
       "          -0.4576,  0.3553,  0.9333,  0.5145, -0.8316,  0.6630,  0.4217,  0.5273,\n",
       "          -0.4060, -0.7656,  0.4512, -0.7154, -0.9978, -0.8190, -0.4839, -0.1566,\n",
       "           0.8559,  0.4302,  0.5985,  0.9542, -0.4781,  0.9163, -0.8885,  0.8662,\n",
       "          -0.4412,  0.3857, -0.8421,  0.8456, -0.0863,  0.6847,  0.0671, -0.9520,\n",
       "          -0.2950, -0.4133, -0.5847, -0.6508, -0.9884, -0.0939, -0.3468, -0.5550,\n",
       "          -0.4797,  0.3580,  0.6474, -0.3197,  0.9686,  0.8022, -0.3577, -0.3660,\n",
       "           0.2667,  0.3316,  0.4105,  0.7940, -0.9595, -0.3607, -0.0616, -0.6573,\n",
       "           0.2050, -0.3518, -0.5584, -0.4846,  0.8459, -0.9682,  0.8277,  0.5278,\n",
       "           0.2143, -0.2243,  0.3813, -0.8014,  0.6003, -0.2025,  0.9532,  0.9991,\n",
       "          -0.5757, -0.6966,  0.9432, -0.9996, -0.6398, -0.4832, -0.5646,  0.0582,\n",
       "          -0.7088,  0.9163,  0.9949,  0.8726, -0.1843, -0.9874,  0.5297, -0.9408,\n",
       "          -0.3772,  0.7696,  0.9965,  0.7667,  0.7055, -0.5630, -0.6963,  0.2588,\n",
       "          -0.9234, -0.8120, -0.9639, -0.4886, -0.8940,  0.9946,  0.5578,  0.9917,\n",
       "          -0.6393, -0.7064, -0.6131, -0.0041,  0.3283,  0.3972, -0.7475, -0.6701,\n",
       "          -0.8239, -0.8362,  0.2952, -0.4013, -0.9768,  0.4279, -0.0595,  0.7017,\n",
       "           0.2120,  0.5918, -0.9966,  0.9352,  1.0000,  0.7209, -0.0094, -0.5165,\n",
       "          -1.0000, -0.9741,  0.9998, -0.9999, -1.0000, -0.2006, -0.7432, -0.0300,\n",
       "          -1.0000, -0.5772, -0.2387,  0.1635,  0.9767,  0.6712, -0.1986, -1.0000,\n",
       "          -0.2898,  0.4014, -0.7422,  0.9976, -0.8261,  0.6693,  0.7682,  0.6021,\n",
       "          -0.4882,  0.6158, -0.9979, -0.4757, -0.9834, -0.9948,  1.0000,  0.3261,\n",
       "          -0.7953,  0.0594,  0.9129, -0.4249,  0.4806, -0.7099, -0.5283,  0.9737,\n",
       "           0.7075,  0.6362,  0.6234,  0.4037,  0.4850,  0.9156, -0.6370,  0.7174,\n",
       "          -0.4287,  0.2115, -0.4048,  0.7851, -0.9676, -0.9293,  0.4103, -0.6790,\n",
       "           0.9928,  1.0000,  0.9387,  0.1006,  0.9076,  0.5138, -0.4870,  1.0000,\n",
       "           0.9462, -0.8415, -0.8123,  0.9553, -0.8269, -0.8409,  0.9844, -0.6063,\n",
       "          -0.9913, -0.9794,  0.9415, -0.8926,  1.0000,  0.2359, -0.6852,  0.6537,\n",
       "           0.4638, -0.9150, -0.0795,  0.3163, -0.9247,  0.4828,  0.6193,  0.8710,\n",
       "           0.5160, -0.2294,  0.0137,  0.3874, -0.7389,  0.4248, -0.9851, -0.7295,\n",
       "           0.9992,  0.4415, -0.3953,  0.1484, -0.5157, -0.6710, -0.5569,  0.9528,\n",
       "           1.0000, -0.6758,  0.9915, -0.5473, -0.2879,  0.6740,  0.7702,  0.7556,\n",
       "          -0.4900,  0.1647,  0.9886, -0.1650, -0.9348, -0.0418,  0.4093, -0.2474,\n",
       "           1.0000,  0.7512,  0.5859,  0.8438,  0.9998,  0.2059, -0.4185,  0.9978,\n",
       "           0.8903, -0.4568,  0.7894, -0.3913, -0.9975, -0.5735, -0.4404,  0.3789,\n",
       "          -0.8143, -0.2713, -0.7016,  0.8166,  0.9994,  0.5481,  0.6404,  0.9971,\n",
       "           1.0000, -0.9933,  0.2061,  0.7862, -0.2857, -1.0000,  0.2050, -0.4991,\n",
       "          -0.2962, -0.9971, -0.5258,  0.4256, -0.7204,  0.9921,  0.9769, -0.5530,\n",
       "          -0.7057, -0.9561,  0.6964,  0.4197, -0.9999, -0.3272, -0.2200,  0.8929,\n",
       "          -0.5070, -0.4451, -0.9543, -0.7286,  0.4786, -0.4748,  0.7314,  0.9966,\n",
       "           0.3246, -0.9968, -0.8025, -0.4307, -0.4238,  0.5816, -0.3111, -0.9994,\n",
       "          -0.5071,  1.0000, -0.6749,  0.9823,  0.0980,  0.4088, -0.4467,  0.3245,\n",
       "           0.9991,  0.7978, -0.9572, -0.9945,  0.8227, -0.7064,  0.7091,  0.9812,\n",
       "           0.9663,  0.2766,  0.9720,  0.5260, -0.4050,  0.6045,  0.7670, -0.2658,\n",
       "          -0.5937, -0.1488, -0.3318, -0.6143,  0.5271,  1.0000,  0.4774,  0.9102,\n",
       "          -0.8909, -0.9930, -0.4327,  1.0000,  0.7106,  0.3832,  0.7861,  0.8854,\n",
       "          -0.5878,  0.1571, -0.5612, -0.4397,  0.5452,  0.1188,  0.6604, -0.6140,\n",
       "          -0.9087, -0.6206,  0.6081, -0.4287,  1.0000, -0.7441, -0.3159, -0.1619,\n",
       "          -0.9355, -0.9769,  0.2602, -0.7748, -0.5461,  0.6946,  0.5719,  0.5694,\n",
       "          -0.8580, -0.6756,  0.9982,  0.9818, -0.9978, -0.5233,  0.5585, -0.3944,\n",
       "           0.8681,  1.0000,  0.6093,  0.9626,  0.3605, -0.1895,  0.6190, -0.9619,\n",
       "           0.4902, -0.1566, -0.5581, -0.4691,  0.7505, -0.5581, -0.9905, -0.0576,\n",
       "           0.5172, -0.6806, -0.7630, -0.3157,  0.5411,  0.7979, -0.4377, -0.4129,\n",
       "           0.5129, -0.3004, -0.1451, -0.6548, -0.7777, -1.0000,  0.4826, -1.0000,\n",
       "           0.9885,  0.8483, -0.3284,  0.4166,  0.7816,  0.9502, -0.0175, -0.9980,\n",
       "          -0.7522,  0.2962, -0.6143, -0.9449, -0.0340,  0.6634, -0.4725,  0.4691,\n",
       "          -0.9863,  0.4042, -0.6128,  1.0000,  0.4343, -0.8319, -0.1370,  0.4560,\n",
       "          -0.6141,  1.0000,  0.2417, -0.7367,  0.4709, -0.9245, -0.4720,  0.6708,\n",
       "           0.5272, -0.7788, -0.9986, -0.4277, -0.0661, -0.7826,  0.7738, -0.6376,\n",
       "          -0.5361,  0.4589,  0.9990,  0.8273,  0.4487,  0.1550, -0.9731, -0.7593,\n",
       "           0.6441,  0.6129, -0.5336,  0.5361,  1.0000,  0.5895, -0.5245, -0.5724,\n",
       "          -0.1300, -0.5179, -0.1892,  0.6093,  0.6154,  0.7406, -0.4710,  0.4524,\n",
       "          -0.9970,  0.2382, -0.9608, -0.9831,  0.5431, -0.4358, -0.7693, -0.8089,\n",
       "           0.9552, -0.1615, -0.1923,  0.5647,  0.2757,  0.6367,  0.5367, -1.0000,\n",
       "           0.7204,  0.5197,  0.9934,  0.5572,  0.9474,  0.9238,  0.6147, -0.6692,\n",
       "           0.1610, -0.5215, -0.3290,  0.3669,  0.8288,  0.2907,  0.3304, -0.3605,\n",
       "          -0.7333, -0.9875, -0.9652, -0.8947,  0.6916, -0.9773,  0.4155,  0.7063,\n",
       "           0.2559, -0.2619, -0.8694, -0.9962, -0.6396, -0.0667, -0.1032,  0.3562,\n",
       "           0.0859, -0.1415,  0.1499,  0.7495, -0.9940,  0.0642, -0.9906,  0.5129,\n",
       "           0.9718, -0.6393,  0.5311,  0.9136, -0.5839,  0.5215, -0.6449,  0.5607,\n",
       "           0.8564, -0.6889,  0.6099, -0.6063, -0.5211, -0.4318, -0.4721, -0.2407,\n",
       "          -0.9029,  0.6822,  0.4809,  0.2508,  0.9866, -0.4421, -0.7365, -0.4479,\n",
       "          -0.9835, -0.5702, -0.2612, -0.3678, -0.8482,  0.9823,  0.2877,  0.9807,\n",
       "           0.7648, -0.4938, -0.4761, -0.5030, -0.2865, -0.8919, -0.7988, -0.7653,\n",
       "           0.4226,  0.4619,  1.0000, -0.9811, -0.9951, -0.8231, -0.5602,  0.5172,\n",
       "          -0.8671, -1.0000,  0.4689, -0.9801,  0.9764, -0.9600,  0.9963, -0.9752,\n",
       "           0.0565, -0.6190,  0.7312,  0.9909, -0.6298, -0.8602,  0.7195, -0.9592,\n",
       "           0.9996, -0.1535, -0.9266, -0.6947,  0.7782, -0.9972, -0.4491, -0.2303]],\n",
       "        device='cuda:0', grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10d17b5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10d17b5c",
    "outputId": "d5dc0acd-77e7-4054-b455-19343ff37951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized BERT (metric drop): 128.33339850010816 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "    times.append(time.perf_counter()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized BERT (metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec6674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f71a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_reload?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a44d3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_reload.device?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c89e2a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in /usr/local/lib/python3.8/dist-packages (13.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich) (2.14.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich) (4.4.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from rich) (2.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a63206c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HuggingFaceInferenceLearner</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">network_parameters</span><span style=\"color: #000080; text-decoration-color: #000080\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModelParams</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span><span style=\"color: #000080; text-decoration-color: #000080\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">input_infos</span><span style=\"color: #000080; text-decoration-color: #000080\">=</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">nebullvm.tools.base.Inp</span><span style=\"color: #000080; text-decoration-color: #000080\">â”€â•®</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"font-weight: bold\">(</span>*args, **kwargs<span style=\"font-weight: bold\">)</span>:                                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080\">Class wrapping an InferenceLearner model and giving to it the</span>                                                   <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080\">huggingface interface.</span>                                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>            <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">CORE_MODEL_SAVE_DIR</span> = <span style=\"color: #008000; text-decoration-color: #008000\">'core_model'</span>                                                                   <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                         <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">device</span> = <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                     <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">input_data</span> = <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                   <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">input_format</span> = <span style=\"color: #008000; text-decoration-color: #008000\">'.pt'</span>                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                    <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">input_names</span> = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_type_ids'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span><span style=\"font-weight: bold\">]</span>                              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                     <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">input_tfms</span> = <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>             <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">network_parameters</span> = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModelParams</span><span style=\"font-weight: bold\">(</span>                                                                   <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                      <span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,                                                              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                      <span style=\"color: #808000; text-decoration-color: #808000\">input_infos</span>=<span style=\"font-weight: bold\">[</span>                                                              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                          <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">nebullvm.tools.base.InputInfo</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fb1251c2100</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #000000; text-decoration-color: #000000\">        &lt;nebullvm.tools.base.InputInfo object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fb1251c2d30</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #000000; text-decoration-color: #000000\">        &lt;nebullvm.tools.base.InputInfo object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fb1251c26a0</span><span style=\"font-weight: bold\">&gt;</span>               <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                      <span style=\"font-weight: bold\">]</span>,                                                                         <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                      <span style=\"color: #808000; text-decoration-color: #808000\">output_sizes</span>=<span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"font-weight: bold\">]]</span>,                                           <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                      <span style=\"color: #808000; text-decoration-color: #808000\">dynamic_info</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DynamicAxisInfo</span><span style=\"font-weight: bold\">(</span>                                              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                          <span style=\"color: #808000; text-decoration-color: #808000\">inputs</span>=<span style=\"font-weight: bold\">[</span>                                                               <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                              <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num_tokens'</span><span style=\"font-weight: bold\">}</span>,                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                              <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num_tokens'</span><span style=\"font-weight: bold\">}</span>                                  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                          <span style=\"font-weight: bold\">]</span>,                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                          <span style=\"color: #808000; text-decoration-color: #808000\">outputs</span>=<span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'batch'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num_tokens'</span><span style=\"font-weight: bold\">}]</span>                            <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                      <span style=\"font-weight: bold\">)</span>                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"font-weight: bold\">)</span>                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                  <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">output_format</span> = <span style=\"color: #008000; text-decoration-color: #008000\">'.pt'</span>                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>               <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">output_structure</span> = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OrderedDict</span><span style=\"font-weight: bold\">([(</span><span style=\"color: #008000; text-decoration-color: #008000\">'output_0'</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'output_1'</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)])</span>                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>         <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">core_inference_learner</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">core_inference_learner</span><span style=\"font-weight: bold\">(</span>*args, **kwargs<span style=\"font-weight: bold\">)</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Model run with Microsoft's </span>       <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">onnxruntime using a Pytorch interface.</span>                                         <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                        <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">forward</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">forward</span><span style=\"font-weight: bold\">(</span>*args, **kwargs<span style=\"font-weight: bold\">)</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Alternative method to the predict one.</span>           <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>             <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">get_inputs_example</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">get_inputs_example</span><span style=\"font-weight: bold\">()</span>:                                                      <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">The function returns an example of the input for the optimized</span>                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model predict method.</span>                                                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                       <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">get_size</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">get_size</span><span style=\"font-weight: bold\">()</span>:                                                                <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                    <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">list2tensor</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">list2tensor</span><span style=\"font-weight: bold\">(</span>listified_tensor: List<span style=\"font-weight: bold\">)</span> -&gt; Any: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Convert list to tensor.</span>        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                           <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">load</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">load</span><span style=\"font-weight: bold\">(</span>path: Union<span style=\"font-weight: bold\">[</span>pathlib.Path, str<span style=\"font-weight: bold\">]</span>, **kwargs<span style=\"font-weight: bold\">)</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Load the model.</span>            <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                    <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">output_type</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">class </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">output_type</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">iterable</span>=<span style=\"font-weight: bold\">()</span>, <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Built-in immutable sequence.</span>                <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                        <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">predict</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">predict</span><span style=\"font-weight: bold\">(</span>*args, **kwargs<span style=\"font-weight: bold\">)</span> -&gt; Any: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Take as input a tensor and returns a </span>     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">prediction</span>                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>             <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">predict_from_files</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">predict_from_files</span><span style=\"font-weight: bold\">(</span>input_files: List<span style=\"font-weight: bold\">[</span>str<span style=\"font-weight: bold\">]</span>, output_files: List<span style=\"font-weight: bold\">[</span>str<span style=\"font-weight: bold\">])</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Get a</span> <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model prediction from file.</span>                                                    <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">predict_from_listified_tensors</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">predict_from_listified_tensors</span><span style=\"font-weight: bold\">(</span>*listified_tensors: List<span style=\"font-weight: bold\">)</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Predict from </span>    <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">listified tensor.</span>                                                              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                            <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">run</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">run</span><span style=\"font-weight: bold\">(</span>*args, **kwargs<span style=\"font-weight: bold\">)</span> -&gt; Any: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Run the underlying optimized model for </span>       <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">getting a prediction.</span>                                                          <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                           <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">save</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">save</span><span style=\"font-weight: bold\">(</span>path: Union<span style=\"font-weight: bold\">[</span>str, pathlib.Path<span style=\"font-weight: bold\">]</span>, **kwargs<span style=\"font-weight: bold\">)</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Save the model.</span>            <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                    <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">tensor2list</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-style: italic\">def </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">tensor2list</span><span style=\"font-weight: bold\">(</span>tensor: Any<span style=\"font-weight: bold\">)</span> -&gt; List: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Convert tensor to list.</span>                  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mâ•­â”€\u001b[0m\u001b[34m \u001b[0m\u001b[1;35mHuggingFaceInferenceLearner\u001b[0m\u001b[1;34m(\u001b[0m\u001b[33mnetwork_parameters\u001b[0m\u001b[34m=\u001b[0m\u001b[1;35mModelParams\u001b[0m\u001b[1;34m(\u001b[0m\u001b[33mbatch_size\u001b[0m\u001b[34m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[34m, \u001b[0m\u001b[33minput_infos\u001b[0m\u001b[34m=\u001b[0m\u001b[1;34m[\u001b[0m\u001b[1;34m<\u001b[0m\u001b[1;95mnebullvm.tools.base.Inp\u001b[0m\u001b[34mâ”€â•®\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m \u001b[3;96mdef \u001b[0m\u001b[1m(\u001b[0m*args, **kwargs\u001b[1m)\u001b[0m:                                                                                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m \u001b[36mClass wrapping an InferenceLearner model and giving to it the\u001b[0m                                                   \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m \u001b[36mhuggingface interface.\u001b[0m                                                                                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m            \u001b[3;33mCORE_MODEL_SAVE_DIR\u001b[0m = \u001b[32m'core_model'\u001b[0m                                                                   \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                         \u001b[3;33mdevice\u001b[0m = \u001b[3;35mNone\u001b[0m                                                                           \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                     \u001b[3;33minput_data\u001b[0m = \u001b[3;35mNone\u001b[0m                                                                           \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                   \u001b[3;33minput_format\u001b[0m = \u001b[32m'.pt'\u001b[0m                                                                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                    \u001b[3;33minput_names\u001b[0m = \u001b[1m[\u001b[0m\u001b[32m'input_ids'\u001b[0m, \u001b[32m'token_type_ids'\u001b[0m, \u001b[32m'attention_mask'\u001b[0m\u001b[1m]\u001b[0m                              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                     \u001b[3;33minput_tfms\u001b[0m = \u001b[3;35mNone\u001b[0m                                                                           \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m             \u001b[3;33mnetwork_parameters\u001b[0m = \u001b[1;35mModelParams\u001b[0m\u001b[1m(\u001b[0m                                                                   \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                      \u001b[33mbatch_size\u001b[0m=\u001b[1;36m1\u001b[0m,                                                              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                      \u001b[33minput_infos\u001b[0m=\u001b[1m[\u001b[0m                                                              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                          \u001b[1m<\u001b[0m\u001b[1;95mnebullvm.tools.base.InputInfo\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7fb1251c2100\u001b[0m\u001b[39m>,\u001b[0m              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[39m        <nebullvm.tools.base.InputInfo object at \u001b[0m\u001b[1;36m0x7fb1251c2d30\u001b[0m\u001b[39m>,\u001b[0m              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[39m        <nebullvm.tools.base.InputInfo object at \u001b[0m\u001b[1;36m0x7fb1251c26a0\u001b[0m\u001b[1m>\u001b[0m               \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                      \u001b[1m]\u001b[0m,                                                                         \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                      \u001b[33moutput_sizes\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m91\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,                                           \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                      \u001b[33mdynamic_info\u001b[0m=\u001b[1;35mDynamicAxisInfo\u001b[0m\u001b[1m(\u001b[0m                                              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                          \u001b[33minputs\u001b[0m=\u001b[1m[\u001b[0m                                                               \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                              \u001b[1m{\u001b[0m\u001b[32m'0'\u001b[0m: \u001b[32m'batch'\u001b[0m, \u001b[32m'1'\u001b[0m: \u001b[32m'num_tokens'\u001b[0m\u001b[1m}\u001b[0m,                                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                              \u001b[1m{\u001b[0m\u001b[32m'0'\u001b[0m: \u001b[32m'batch'\u001b[0m, \u001b[32m'1'\u001b[0m: \u001b[32m'num_tokens'\u001b[0m\u001b[1m}\u001b[0m                                  \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                          \u001b[1m]\u001b[0m,                                                                     \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                          \u001b[33moutputs\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'0'\u001b[0m: \u001b[32m'batch'\u001b[0m, \u001b[32m'1'\u001b[0m: \u001b[32m'num_tokens'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m                            \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                      \u001b[1m)\u001b[0m                                                                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[1m)\u001b[0m                                                                              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                  \u001b[3;33moutput_format\u001b[0m = \u001b[32m'.pt'\u001b[0m                                                                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m               \u001b[3;33moutput_structure\u001b[0m = \u001b[1;35mOrderedDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'output_0'\u001b[0m, \u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'output_1'\u001b[0m, \u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m         \u001b[3;33mcore_inference_learner\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mcore_inference_learner\u001b[0m\u001b[1m(\u001b[0m*args, **kwargs\u001b[1m)\u001b[0m: \u001b[2mModel run with Microsoft's \u001b[0m       \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[2monnxruntime using a Pytorch interface.\u001b[0m                                         \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                        \u001b[3;33mforward\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mforward\u001b[0m\u001b[1m(\u001b[0m*args, **kwargs\u001b[1m)\u001b[0m: \u001b[2mAlternative method to the predict one.\u001b[0m           \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m             \u001b[3;33mget_inputs_example\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mget_inputs_example\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:                                                      \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[2mThe function returns an example of the input for the optimized\u001b[0m                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[2mmodel predict method.\u001b[0m                                                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                       \u001b[3;33mget_size\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mget_size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:                                                                \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                    \u001b[3;33mlist2tensor\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mlist2tensor\u001b[0m\u001b[1m(\u001b[0mlistified_tensor: List\u001b[1m)\u001b[0m -> Any: \u001b[2mConvert list to tensor.\u001b[0m        \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                           \u001b[3;33mload\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mload\u001b[0m\u001b[1m(\u001b[0mpath: Union\u001b[1m[\u001b[0mpathlib.Path, str\u001b[1m]\u001b[0m, **kwargs\u001b[1m)\u001b[0m: \u001b[2mLoad the model.\u001b[0m            \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                    \u001b[3;33moutput_type\u001b[0m = \u001b[3;96mclass \u001b[0m\u001b[1;31moutput_type\u001b[0m\u001b[1m(\u001b[0m\u001b[33miterable\u001b[0m=\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[35m/\u001b[0m\u001b[1m)\u001b[0m: \u001b[2mBuilt-in immutable sequence.\u001b[0m                \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                        \u001b[3;33mpredict\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mpredict\u001b[0m\u001b[1m(\u001b[0m*args, **kwargs\u001b[1m)\u001b[0m -> Any: \u001b[2mTake as input a tensor and returns a \u001b[0m     \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[2mprediction\u001b[0m                                                                     \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m             \u001b[3;33mpredict_from_files\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mpredict_from_files\u001b[0m\u001b[1m(\u001b[0minput_files: List\u001b[1m[\u001b[0mstr\u001b[1m]\u001b[0m, output_files: List\u001b[1m[\u001b[0mstr\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m: \u001b[2mGet a\u001b[0m \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[2mmodel prediction from file.\u001b[0m                                                    \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m \u001b[3;33mpredict_from_listified_tensors\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mpredict_from_listified_tensors\u001b[0m\u001b[1m(\u001b[0m*listified_tensors: List\u001b[1m)\u001b[0m: \u001b[2mPredict from \u001b[0m    \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[2mlistified tensor.\u001b[0m                                                              \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                            \u001b[3;33mrun\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mrun\u001b[0m\u001b[1m(\u001b[0m*args, **kwargs\u001b[1m)\u001b[0m -> Any: \u001b[2mRun the underlying optimized model for \u001b[0m       \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                  \u001b[2mgetting a prediction.\u001b[0m                                                          \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                           \u001b[3;33msave\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31msave\u001b[0m\u001b[1m(\u001b[0mpath: Union\u001b[1m[\u001b[0mstr, pathlib.Path\u001b[1m]\u001b[0m, **kwargs\u001b[1m)\u001b[0m: \u001b[2mSave the model.\u001b[0m            \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                    \u001b[3;33mtensor2list\u001b[0m = \u001b[3;96mdef \u001b[0m\u001b[1;31mtensor2list\u001b[0m\u001b[1m(\u001b[0mtensor: Any\u001b[1m)\u001b[0m -> List: \u001b[2mConvert tensor to list.\u001b[0m                  \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rich\n",
    "rich.inspect(optimized_model_reload, methods=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37aeda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_reload.device = 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3581f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_reload.core_inference_learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b26b2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for original BERT: 32.61423967003793 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_reload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "523dce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average response time for optimized BERT (metric drop): 35.351582590192265 ms\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "# Warmup for 30 iterations\n",
    "for encoded_input in encoded_inputs[:30]:\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model_reload(**encoded_input)\n",
    "\n",
    "# Benchmark\n",
    "for encoded_input in encoded_inputs:\n",
    "    st = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model_reload(**encoded_input)\n",
    "    times.append(time.perf_counter()-st)\n",
    "optimized_model_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for optimized BERT (metric drop): {optimized_model_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bf3d1fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bf3d1fb",
    "outputId": "6163d8ba-254f-47d2-a468-a921622a15ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3945, -0.2617,  0.2220,  ..., -0.8755,  1.0244,  0.1929],\n",
       "          [-0.1223,  1.0918,  0.7598,  ..., -0.7944,  1.2998,  0.3601],\n",
       "          [-1.6465, -0.3750,  0.5508,  ...,  0.0803,  0.6504,  0.6123],\n",
       "          ...,\n",
       "          [-0.6943, -0.5884, -0.1506,  ..., -0.2715,  0.7266, -0.5283],\n",
       "          [-1.0371, -1.0156, -0.3401,  ...,  0.5532,  0.9189, -0.3748],\n",
       "          [ 0.1237,  0.1702, -0.2141,  ...,  0.4604, -0.2036, -0.1069]]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[-0.4470, -0.7217, -0.9985,  0.8179,  0.9683, -0.6338, -0.2499,  0.6436,\n",
       "          -0.9946, -0.9990, -0.9268,  0.9946,  0.6553,  0.9038, -0.1532, -0.7642,\n",
       "          -0.7588, -0.6338,  0.3765,  0.9082,  0.4102,  1.0000, -0.8799,  0.6670,\n",
       "           0.5952,  0.9995, -0.8525,  0.4082,  0.6069,  0.4709, -0.2269,  0.5078,\n",
       "          -0.9507, -0.3203, -0.9995, -0.8730,  0.7808,  0.2542, -0.1000, -0.2391,\n",
       "          -0.0997,  0.6548,  1.0000,  0.0044,  0.8164, -0.0666, -1.0000,  0.5781,\n",
       "          -0.1704,  0.9980,  0.9922,  0.9985,  0.5610,  0.5625,  0.7520, -0.9058,\n",
       "           0.1223,  0.3972, -0.5898, -0.5171, -0.5327,  0.5371, -0.9897,  0.0600,\n",
       "           0.9990,  0.9932, -0.6479, -0.5342, -0.2734, -0.0874, -0.1204,  0.3918,\n",
       "          -0.5977, -0.4241,  0.9844,  0.5898, -0.6772,  1.0000, -0.7437, -0.8257,\n",
       "           0.9966,  0.9878,  0.7949, -0.9478,  0.9653, -1.0000,  0.7930, -0.4250,\n",
       "          -0.7944,  0.5112,  0.6880, -0.4829,  0.9795,  0.7783, -0.6436, -0.8613,\n",
       "          -0.5156, -0.9980, -0.5547, -0.7451,  0.4871, -0.5537, -0.7417, -0.3535,\n",
       "           0.5972, -0.7139,  0.1296,  0.8306,  0.5752,  0.3555,  0.6968, -0.5933,\n",
       "           0.7046, -0.5054,  0.6040, -0.4609, -0.8867, -0.7153, -0.8247,  0.1552,\n",
       "          -0.7808, -0.5039, -0.1063, -0.9258,  0.7656, -0.5786, -0.9990, -1.0000,\n",
       "          -0.8735, -0.8760, -0.6616, -0.5698, -0.8101, -0.8159,  0.6152,  0.4727,\n",
       "           0.5244,  1.0000, -0.6357,  0.6333, -0.8350, -0.9585,  0.9785, -0.5674,\n",
       "           0.9629, -0.1232,  0.0248,  0.2340, -0.9141,  0.7666, -0.9580, -0.4878,\n",
       "          -0.9868, -0.1252, -0.2954,  0.5400, -0.9663, -0.9990, -0.8848, -0.2878,\n",
       "          -0.4573,  0.3552,  0.9331,  0.5142, -0.8311,  0.6626,  0.4202,  0.5269,\n",
       "          -0.4060, -0.7651,  0.4507, -0.7148, -0.9976, -0.8193, -0.4836, -0.1569,\n",
       "           0.8560,  0.4302,  0.5981,  0.9541, -0.4778,  0.9160, -0.8887,  0.8667,\n",
       "          -0.4407,  0.3853, -0.8418,  0.8452, -0.0864,  0.6841,  0.0665, -0.9517,\n",
       "          -0.2954, -0.4128, -0.5845, -0.6504, -0.9883, -0.0939, -0.3464, -0.5547,\n",
       "          -0.4792,  0.3591,  0.6465, -0.3198,  0.9683,  0.8018, -0.3579, -0.3655,\n",
       "           0.2661,  0.3311,  0.4097,  0.7944, -0.9595, -0.3601, -0.0615, -0.6577,\n",
       "           0.2046, -0.3521, -0.5576, -0.4844,  0.8457, -0.9683,  0.8267,  0.5273,\n",
       "           0.2155, -0.2244,  0.3809, -0.8013,  0.5996, -0.2024,  0.9531,  0.9990,\n",
       "          -0.5757, -0.6973,  0.9434, -0.9995, -0.6396, -0.4832, -0.5645,  0.0573,\n",
       "          -0.7085,  0.9165,  0.9951,  0.8721, -0.1852, -0.9873,  0.5303, -0.9404,\n",
       "          -0.3767,  0.7690,  0.9966,  0.7661,  0.7051, -0.5620, -0.6963,  0.2573,\n",
       "          -0.9233, -0.8125, -0.9639, -0.4878, -0.8940,  0.9946,  0.5576,  0.9917,\n",
       "          -0.6387, -0.7061, -0.6138, -0.0053,  0.3279,  0.3965, -0.7471, -0.6689,\n",
       "          -0.8232, -0.8364,  0.2949, -0.4006, -0.9766,  0.4275, -0.0601,  0.7017,\n",
       "           0.2118,  0.5913, -0.9966,  0.9351,  1.0000,  0.7212, -0.0087, -0.5171,\n",
       "          -1.0000, -0.9741,  1.0000, -1.0000, -1.0000, -0.2010, -0.7427, -0.0300,\n",
       "          -1.0000, -0.5767, -0.2385,  0.1630,  0.9766,  0.6719, -0.1996, -1.0000,\n",
       "          -0.2888,  0.4016, -0.7417,  0.9976, -0.8257,  0.6699,  0.7676,  0.6021,\n",
       "          -0.4878,  0.6152, -0.9980, -0.4753, -0.9834, -0.9946,  1.0000,  0.3254,\n",
       "          -0.7949,  0.0589,  0.9126, -0.4243,  0.4800, -0.7104, -0.5278,  0.9736,\n",
       "           0.7070,  0.6357,  0.6230,  0.4038,  0.4846,  0.9155, -0.6372,  0.7168,\n",
       "          -0.4294,  0.2118, -0.4041,  0.7847, -0.9673, -0.9292,  0.4106, -0.6787,\n",
       "           0.9927,  1.0000,  0.9385,  0.1001,  0.9072,  0.5132, -0.4866,  1.0000,\n",
       "           0.9463, -0.8418, -0.8120,  0.9551, -0.8267, -0.8408,  0.9844, -0.6060,\n",
       "          -0.9912, -0.9795,  0.9419, -0.8931,  1.0000,  0.2360, -0.6855,  0.6538,\n",
       "           0.4646, -0.9146, -0.0800,  0.3159, -0.9243,  0.4824,  0.6191,  0.8706,\n",
       "           0.5156, -0.2286,  0.0140,  0.3884, -0.7388,  0.4246, -0.9849, -0.7285,\n",
       "           0.9990,  0.4409, -0.3950,  0.1478, -0.5151, -0.6714, -0.5571,  0.9526,\n",
       "           1.0000, -0.6753,  0.9917, -0.5464, -0.2874,  0.6733,  0.7700,  0.7554,\n",
       "          -0.4895,  0.1644,  0.9888, -0.1639, -0.9351, -0.0423,  0.4089, -0.2471,\n",
       "           1.0000,  0.7505,  0.5854,  0.8433,  1.0000,  0.2052, -0.4192,  0.9976,\n",
       "           0.8906, -0.4565,  0.7891, -0.3921, -0.9976, -0.5732, -0.4402,  0.3787,\n",
       "          -0.8145, -0.2710, -0.7021,  0.8169,  0.9995,  0.5479,  0.6401,  0.9971,\n",
       "           1.0000, -0.9932,  0.2057,  0.7866, -0.2869, -1.0000,  0.2053, -0.4988,\n",
       "          -0.2954, -0.9971, -0.5254,  0.4253, -0.7207,  0.9922,  0.9771, -0.5518,\n",
       "          -0.7061, -0.9561,  0.6958,  0.4194, -1.0000, -0.3267, -0.2202,  0.8926,\n",
       "          -0.5063, -0.4453, -0.9541, -0.7285,  0.4783, -0.4746,  0.7310,  0.9966,\n",
       "           0.3259, -0.9966, -0.8022, -0.4302, -0.4233,  0.5806, -0.3108, -0.9995,\n",
       "          -0.5068,  1.0000, -0.6748,  0.9824,  0.0980,  0.4087, -0.4463,  0.3240,\n",
       "           0.9990,  0.7979, -0.9570, -0.9946,  0.8232, -0.7061,  0.7085,  0.9810,\n",
       "           0.9663,  0.2769,  0.9722,  0.5254, -0.4045,  0.6040,  0.7661, -0.2651,\n",
       "          -0.5933, -0.1481, -0.3315, -0.6138,  0.5278,  1.0000,  0.4771,  0.9102,\n",
       "          -0.8911, -0.9932, -0.4321,  1.0000,  0.7109,  0.3831,  0.7861,  0.8853,\n",
       "          -0.5874,  0.1559, -0.5605, -0.4392,  0.5449,  0.1182,  0.6611, -0.6138,\n",
       "          -0.9092, -0.6201,  0.6074, -0.4297,  1.0000, -0.7437, -0.3154, -0.1613,\n",
       "          -0.9355, -0.9771,  0.2598, -0.7754, -0.5454,  0.6943,  0.5728,  0.5688,\n",
       "          -0.8579, -0.6758,  0.9980,  0.9819, -0.9980, -0.5239,  0.5596, -0.3945,\n",
       "           0.8677,  1.0000,  0.6089,  0.9624,  0.3599, -0.1891,  0.6187, -0.9619,\n",
       "           0.4895, -0.1571, -0.5576, -0.4688,  0.7505, -0.5576, -0.9902, -0.0573,\n",
       "           0.5166, -0.6802, -0.7627, -0.3147,  0.5410,  0.7979, -0.4373, -0.4124,\n",
       "           0.5127, -0.2998, -0.1459, -0.6543, -0.7773, -1.0000,  0.4822, -1.0000,\n",
       "           0.9883,  0.8477, -0.3279,  0.4167,  0.7817,  0.9502, -0.0180, -0.9980,\n",
       "          -0.7510,  0.2961, -0.6138, -0.9448, -0.0345,  0.6631, -0.4722,  0.4688,\n",
       "          -0.9863,  0.4038, -0.6128,  1.0000,  0.4336, -0.8315, -0.1360,  0.4556,\n",
       "          -0.6138,  1.0000,  0.2428, -0.7373,  0.4705, -0.9243, -0.4719,  0.6704,\n",
       "           0.5269, -0.7783, -0.9985, -0.4275, -0.0679, -0.7822,  0.7734, -0.6372,\n",
       "          -0.5356,  0.4583,  0.9990,  0.8276,  0.4487,  0.1545, -0.9731, -0.7593,\n",
       "           0.6445,  0.6128, -0.5337,  0.5356,  1.0000,  0.5889, -0.5249, -0.5718,\n",
       "          -0.1311, -0.5176, -0.1893,  0.6089,  0.6152,  0.7407, -0.4707,  0.4534,\n",
       "          -0.9971,  0.2379, -0.9604, -0.9829,  0.5430, -0.4370, -0.7700, -0.8096,\n",
       "           0.9551, -0.1610, -0.1919,  0.5645,  0.2751,  0.6362,  0.5366, -1.0000,\n",
       "           0.7207,  0.5190,  0.9932,  0.5581,  0.9473,  0.9238,  0.6143, -0.6699,\n",
       "           0.1620, -0.5210, -0.3286,  0.3667,  0.8286,  0.2910,  0.3298, -0.3601,\n",
       "          -0.7329, -0.9873, -0.9653, -0.8950,  0.6914, -0.9771,  0.4163,  0.7065,\n",
       "           0.2544, -0.2615, -0.8691, -0.9961, -0.6401, -0.0669, -0.1034,  0.3557,\n",
       "           0.0861, -0.1406,  0.1492,  0.7500, -0.9941,  0.0630, -0.9907,  0.5122,\n",
       "           0.9717, -0.6396,  0.5308,  0.9136, -0.5830,  0.5210, -0.6445,  0.5605,\n",
       "           0.8564, -0.6890,  0.6094, -0.6064, -0.5205, -0.4314, -0.4717, -0.2402,\n",
       "          -0.9028,  0.6816,  0.4802,  0.2515,  0.9863, -0.4417, -0.7363, -0.4475,\n",
       "          -0.9834, -0.5708, -0.2622, -0.3674, -0.8477,  0.9824,  0.2871,  0.9810,\n",
       "           0.7642, -0.4934, -0.4753, -0.5029, -0.2871, -0.8916, -0.7983, -0.7651,\n",
       "           0.4224,  0.4617,  1.0000, -0.9810, -0.9951, -0.8228, -0.5596,  0.5166,\n",
       "          -0.8672, -1.0000,  0.4683, -0.9800,  0.9766, -0.9600,  0.9961, -0.9751,\n",
       "           0.0568, -0.6187,  0.7314,  0.9907, -0.6294, -0.8599,  0.7188, -0.9590,\n",
       "           0.9995, -0.1534, -0.9263, -0.6938,  0.7778, -0.9971, -0.4487, -0.2299]],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2663c",
   "metadata": {
    "id": "ceb60d8c"
   },
   "source": [
    "## Save and reload the optimized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eda1a0",
   "metadata": {},
   "source": [
    "We can easily save to disk the optimized model with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d84bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save(\"optimized_model\")\n",
    "\n",
    "from nebullvm.operations.inference_learners.base import LearnerMetadata\n",
    "\n",
    "optimized_model_reload = LearnerMetadata.read(\"optimized_model\").load_model(\"optimized_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62b6fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save(\"model_save_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c968d51",
   "metadata": {},
   "source": [
    "We can then load again the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c1340c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebullvm.operations.inference_learners.base import LearnerMetadata\n",
    "\n",
    "optimized_model = LearnerMetadata.read(\"model_save_path\").load_model(\"model_save_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc8bd143",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceInferenceLearner' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimized_model_cuda \u001b[38;5;241m=\u001b[39m \u001b[43moptimized_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceInferenceLearner' object has no attribute 'to'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb234e5e",
   "metadata": {
    "id": "cb234e5e"
   },
   "source": [
    "Great! Was it easy? How are the results? Do you have any comments?\n",
    "Share your optimization results and thoughts with <a href=\"https://discord.gg/RbeQMu886J\" target=\"_blank\"> our community on Discord</a>, where we chat about Speedster and AI acceleration.\n",
    "\n",
    "Note that the acceleration of Speedster depends very much on the hardware configuration and your AI model. Given the same input model, Speedster can accelerate it by 10 times on some machines and perform poorly on others.\n",
    "\n",
    "If you want to learn more about how Speedster works, look at other tutorials and performance benchmarks, check out the links below or write to us on Discord."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ff2ac",
   "metadata": {
    "id": "b77ff2ac"
   },
   "source": [
    "<center> \n",
    "    <a href=\"https://discord.com/invite/RbeQMu886J\" target=\"_blank\" style=\"text-decoration: none;\"> Join the community </a> |\n",
    "    <a href=\"https://nebuly.gitbook.io/nebuly/welcome/questions-and-contributions\" target=\"_blank\" style=\"text-decoration: none;\"> Contribute to the library </a>\n",
    "</center>\n",
    "\n",
    "<center> \n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#how-it-works\" target=\"_blank\" style=\"text-decoration: none;\"> How nebullvm works </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#documentation\" target=\"_blank\" style=\"text-decoration: none;\"> Documentation </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#api-quick-view\" target=\"_blank\" style=\"text-decoration: none;\"> API quick view </a> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "nebullvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "522e2531dcf80af8b40af221fb3e8d7fe3bdccd86bae1be78502e3de809f4577"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
